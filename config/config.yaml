# config/config.yaml - Updated for Level 3

app:
  name: "RAG Chatbot"
  version:  "3.0.0"
  environment: "production"
  log_level: "INFO"

# ==================== PATHS ====================
paths:
  data_dir: "data/documents"      # Changed to support multiple files
  db_dir: "chroma_db"
  log_file: "rag_system.log"
  metrics_file: "metrics.json"   # File to store metrics

# ==================== PDF PROCESSING ====================
pdf: 
  max_file_size_mb: 100
  allowed_extensions: [".pdf", ".PDF"]
  batch_size: 10                  # Process N files at a time
  skip_existing: true             # Skip if already in DB

# ==================== TEXT SPLITTING ====================
chunking:
  chunk_size:  1000
  chunk_overlap: 200
  separators: ["\n\n", "\n", ". ", " ", ""]
  add_metadata: true              # Add rich metadata to chunks

# ==================== EMBEDDINGS ====================
embeddings: 
  model_name: "all-MiniLM-L6-v2"
  device: "cuda"
  normalize: true
  batch_size:  32                  # Batch embedding for speed

# ==================== VECTOR DATABASE ====================
vectordb:
  type: "chroma"
  collection_name: "documents"
  distance_metric: "cosine"
  
  chroma: 
    anonymized_telemetry: false
    allow_reset: true
  
  hnsw:
    space:  "cosine"
    construction_ef:  100
    search_ef: 100
    M: 16

# ==================== RETRIEVAL ====================
retrieval:
  search_type: "mmr"              # Changed to MMR for diversity
  k: 5                            # Increased from 3
  score_threshold: 0.0
  
  mmr:
    fetch_k: 20                   # Fetch 20 candidates
    lambda_mult: 0.5              # Balance relevance vs diversity
  
  rerank:                         # Re-ranking
    enabled: false                # Will implement later
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_n: 3

# ==================== LLM ====================
llm:
  provider: "ollama"
  model: "llama3"
  temperature: 0.1
  max_tokens: 512
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  
  ollama:
    base_url: "http://localhost:11434"
    timeout: 60

# ==================== PROMPT ====================
prompt:
  system_message: |
    B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh v√† chuy√™n nghi·ªáp c·ªßa c√° nh√¢n t√¥i. 
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
  
  rules:
    - "Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ng·ªØ c·∫£nh"
    - "N·∫øu kh√¥ng bi·∫øt, n√≥i 'T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin'"
    - "Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát r√µ r√†ng"
    - "Tr√≠ch d·∫´n ngu·ªìn (t√™n file, trang s·ªë)"
  
  # Conversation-aware template
  conversation_template: |
    {system_message}
    
    QUY T·∫ÆC: 
    {rules}
    
    L·ªäCH S·ª¨ H·ªòI THO·∫†I:
    {history}
    
    NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU: 
    {context}
    
    C√ÇU H·ªéI M·ªöI:  {question}
    
    TR·∫¢ L·ªúI: 

# ==================== CHAT ====================
chat:
  show_sources: true
  max_query_length: 500
  history_enabled: true           # Enable conversation memory
  max_history:  5                  # Keep last 5 exchanges
  
  source_format: "detailed"       # detailed | compact | none
  
  messages:
    welcome:  "ü§ñ RAG Chatbot v3.0 ƒë√£ s·∫µn s√†ng!"
    exit: "üëã T·∫°m bi·ªát!"
    processing: "‚è≥ ƒêang x·ª≠ l√Ω..."
    error: "‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra"

# ==================== LOGGING ====================
logging:
  level: "INFO"
  format:  "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  handlers:
    console:
      enabled: true
      level: "INFO"
    
    file:
      enabled: true
      level: "DEBUG"
      max_bytes: 10485760
      backup_count: 5

# ==================== METRICS ====================
metrics:                          # ‚Üê NEW
  enabled: true
  track: 
    - "query_count"
    - "response_time"
    - "retrieval_time"
    - "generation_time"
    - "avg_similarity_score"
  
  save_interval: 10               # Save every 10 queries