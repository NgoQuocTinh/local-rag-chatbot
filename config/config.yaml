# config/config.yaml - Updated for Level 3

app:
  name: "RAG Chatbot"
  version:  "3.0.0"
  environment: "production"
  log_level: "INFO"

# ==================== PATHS ====================
paths:
  data_dir: "data/documents"      # Changed to support multiple files
  db_dir: "chroma_db"
  log_file: "rag_system.log"
  metrics_file: "metrics.json"   # File to store metrics

# ==================== PDF PROCESSING ====================
pdf: 
  max_file_size_mb: 100
  allowed_extensions: [".pdf", ".PDF"]
  batch_size: 10                  # Process N files at a time
  skip_existing: true             # Skip if already in DB

# ==================== TEXT SPLITTING ====================
# Large chunk size for a documents with many letters (like a book, a article, a report, etc.)
# Small chunk size for a documents with few letters (like a email, a message, a short text, etc.)
# Large chunk overlap for a documents with many letters (like a book, a article, a report, etc.)
# Small chunk overlap for a documents with few letters (like a email, a message, a short text, etc.)
chunking:
  chunk_size:  1200 # for a documents with many letters (like a book, a article, a report, etc.)
  chunk_overlap: 250 # for a documents with many letters (like a book, a article, a report, etc.)
  separators: ["\n\n", "\n", ". ", " ", ""] # for a documents with many letters (like a book, a article, a report, etc.)
  add_metadata: true              # Add rich metadata to chunks

# ==================== EMBEDDINGS ====================
embeddings: 
  model_name: "paraphrase-multilingual-mpnet-base-v2" # all-MiniLM-L6-v2; all-mpnet-base-v2: for english text
  device: "cuda"
  normalize: true
  batch_size:  32                  # Batch embedding for speed

# ==================== VECTOR DATABASE ====================
vectordb:
  type: "chroma"
  collection_name: "documents"
  distance_metric: "cosine"
  
  chroma: 
    anonymized_telemetry: false
    allow_reset: true
  
  hnsw:
    space:  "cosine"
    construction_ef:  100
    search_ef: 100
    M: 16

# ==================== RETRIEVAL ====================
retrieval:
# mmr: Balance relevance (s·ª± t∆∞∆°ng ƒë·ªìng) vs diversity (s·ª± ƒëa d·∫°ng). it gonna combine from Doc A, Doc B,.... to get the most relevant documents and response
# similarity: Balance relevance (s·ª± t∆∞∆°ng ƒë·ªìng) vs diversity (s·ª± ƒëa d·∫°ng)
  search_type: "mmr"          # Changed to MMR for diversity; mmr: Maximum Marginal Relevance, use it when you want to retrieve the most relevant documents, 
  # similarity: "similarity"  # Similarity Search, use it when you want to retrieve the most similar documents
  k: 4                        # Increased from 3; k: Number of documents to retrieve
  score_threshold: 0.6        # Score threshold; 0.0: No threshold, 1.0: All documents. If it too small, it gonna return with fake documents
  
  # Similarity configuration
  # similarity:
  #   k: 5                            # Increased from 3; k: Number of documents to retrieve
  #   score_threshold: 0.0            # Score threshold; 0.0: No threshold, 1.0: All documents 

  # MMR configuration
  mmr:
    fetch_k: 20                   # Fetch 20 candidates; fetch_k: Number of documents to fetch
    lambda_mult: 0.8              # Balance relevance vs diversity, 0.0: diversity (ƒëa d·∫°ng), 1.0: relevance (ch√≠nh x√°c tuy·ªát ƒë·ªëi)
  
  # Rerank configuration
  rerank:                         # Re-ranking
    enabled: true                # Will implement later
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_n: 3

# ==================== LLM ====================
llm:
  provider: "ollama"
  model: "llama3"
  temperature: 0.1
  max_tokens: 512
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  
  ollama:
    base_url: "http://localhost:11434"
    timeout: 60

# ==================== PROMPT ====================
prompt:
  system_message: |
    B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh v√† chuy√™n nghi·ªáp c·ªßa c√° nh√¢n t√¥i. 
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
  
  rules:
    - "Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ng·ªØ c·∫£nh"
    - "N·∫øu kh√¥ng bi·∫øt, n√≥i 'T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin'"
    - "Kh√¥ng ƒë∆∞·ª£c suy ƒëo√°n khi thi·∫øu th√¥ng tin"
    - "C√¢u tr·∫£ l·ªùi ph·∫£i ƒë∆∞·ª£c tr√≠ch d·∫´n v√† t·ªïng h·ª£p t·ª´ nhi·ªÅu t·∫£i li·ªáu"
    - "N·∫øu th√¥ng tin c·ªßa c√¢u tr·∫£ l·ªùi xu·∫•t hi·ªán ·ªü nhi·ªÅu n∆°i th√¨ c·∫ßn ph·∫£i t·ªïng h·ª£p th√¥ng tin ƒë√≥ l·∫°i r·ªìi m·ªõi tr·∫£ l·ªüi"
    - "Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát r√µ r√†ng"
    - "Tr√≠ch d·∫´n ngu·ªìn (t√™n file, trang s·ªë)"
    - "Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc, ∆∞u ti√™n tr·ª±c quan, v√≠ d·ª• c·ª• th·ªÉ"
    - "Lu√¥n tr√≠ch d·∫´n ph·∫ßn trong t√†i li·ªáu (t√™n file, ch∆∞∆°ng/m·ª•c, trang) khi ƒë∆∞a ra ƒë·ªãnh nghƒ©a ho·∫∑c c√¥ng th·ª©c"
  
  # Conversation-aware template
  conversation_template: |
    {system_message}
    
    QUY T·∫ÆC: 
    {rules}
    
    L·ªäCH S·ª¨ H·ªòI THO·∫†I:
    {history}
    
    NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU: 
    {context}
    
    C√ÇU H·ªéI M·ªöI:  {question}
    
    TR·∫¢ L·ªúI: 

# ==================== CHAT ====================
chat:
  show_sources: true
  max_query_length: 500
  history_enabled: true           # Enable conversation memory
  max_history:  8                  # Keep last 5 exchanges
  
  source_format: "detailed"       # detailed | compact | none
  
  messages:
    welcome:  "ü§ñ RAG Chatbot v3.0 ƒë√£ s·∫µn s√†ng!"
    exit: "üëã T·∫°m bi·ªát!"
    processing: "‚è≥ ƒêang x·ª≠ l√Ω..."
    error: "‚ùå ƒê√£ c√≥ l·ªói x·∫£y ra"

# ==================== LOGGING ====================
logging:
  level: "INFO"
  format:  "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  handlers:
    console:
      enabled: true
      level: "INFO"
    
    file:
      enabled: true
      level: "DEBUG"
      max_bytes: 10485760
      backup_count: 5

# ==================== METRICS ====================
metrics:                          # ‚Üê NEW
  enabled: true
  track: 
    - "query_count"
    - "response_time"
    - "retrieval_time"
    - "generation_time"
    - "avg_similarity_score"
  
  save_interval: 10               # Save every 10 queries